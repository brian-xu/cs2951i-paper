\section{Results}

We demonstrate the performance of our approach against our baseline, VideoComposer, a controllable video diffusion model. In Table 1, we demonstrate our results through a side-by-side comparison with VideoComposer. As shown, the existing baseline fails to capture the intrinsics of the subject's change in motion, and even in some cases fails to comprehend the motion dynamics of the original driving video. We see in parachute example that SAM-IAM captures the shape of the parachute throughout the length of the projected motion, whereas VideoComposer suffers from shape deformation. Furthermore, the soccer ball example demonstrates the ability of SAM-IAM to comprehend more complex motions as the driving video illustrates a car traveling across the frame from right to left, with the addition of forward motion towards the camera in the beginning frames. However, VideoComposer fails to correctly capture the scale of the input image while adding additional artifacts to the resulting video.
In the equestrian example, we qualitatively compare SAM-IAM and VideoComposer on a driving video that exhibits human motion in the form of jumping. While VideoComposer adds artifacts to the foreground and background of the frame and incorrectly adds rotation to the target object's movement, SAM-IAM cleanly translates the movement of the jumping rollerblader to the horse in the same arcing motion.
In the case of occlusion, SAM-IAM achieves an accurate representation by adding another moving vehicle to obstruct the view of the car, analogous to the way the plant occludes the motorcycle in the driving video. In contrast, VideoComposer exhibits multiple colliding to vehicles to its resulting video, demonstrating a lack of understanding of occlusion. 

compare with video diffusion models and potentially wombo ai if possible?
