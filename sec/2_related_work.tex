\section{Related works}
\label{sec:related_work}

Much related work in video synthesis have tended to fall into either category of image-to-video methods or video diffusion models.

%-------------------------------------------------------------------------
\subsection{Image-to-Video Methods}

Image-to-video methods exist in two flavors: diffusion and non-diffusion based. Periodic patterns have been manipulated to generate seamlessly animated \emph{endless loops} \cite{Halperin_2021} in a non-machine learning approach, while neural network architecture has been employed in a \emph{conditional invertible neural network (cINN)} architecture \cite{dorkenwald2021stochasticimagetovideosynthesisusing}. 

Diffusion based image-to-video methods rely on the remarkable performance of diffusion models in image synthesis by extending the existing text-to-video models with temporal-consistency attention layers for image-to-video synthesis and larger training datasets \cite{blattmann2023stablevideodiffusionscaling}. This approach has seen applications in character animation \cite{hu2024animateanyoneconsistentcontrollable} and fashion posing \cite{52750}. ControlNet \cite{zhang2023addingconditionalcontroltexttoimage} style approaches extended to video have also been succesfully built \cite{zhang2023controlvideotrainingfreecontrollabletexttovideo}.


%-------------------------------------------------------------------------
\subsection{Video Diffusion}

With the success of text-to-image and image-to-video models, diffusion models have been successfully developed for training directly on videos themselves in the video-to-video mode. The major obstacle of temporal consistency and coherence has been tackled in a variety of different ways, typically relying on a pretrained diffusion model supplemented with carefully augmented attention layers and conditioning \cite{melnik2024videodiffusionmodelssurvey}. Controllable video synthesis that allows for user guidance in textual, spatial, and temporal forms have been explored by using motion vectors and spatio-temporal condition encoders \cite{wang2023videocomposercompositionalvideosynthesis}, propagating and injecting condition features through condition adapters \cite{wang2024easycontroltransfercontrolnetvideo}, and explicit motion modeling \cite{shi2024motioni2vconsistentcontrollableimagetovideo}.

\subsection{Conditioning Object Motion}

Conditioning object motion can be viewed as a form of spatio-temporal guidance in video generation. Diffusion-based video generative models supporting motion control can be categorized based on the specificity and type of input they use.

At one end of the spectrum, detailed spatial conditioning sequences are used as frame-by-frame guidance. These spatial constraints, in the form of optical flow \cite{ni2023conditional}, motion vector \cite{2023videocomposer}, pose \cite{feng2023dreamoving}, depth \cite{2023videocomposer, feng2023dreamoving}, and sketch outline \cite{2023videocomposer}, enable precise control over the generated video. Focusing on specific applications, specialized frameworks have been proposed to customize human videos \cite{feng2023dreamoving}. When applied to objects of a different kind, these methods often yield results akin to style transfers. The strong spatial constraint sequences used by these methods limits their capability in adaptively transferring motion properties between different object types.

On the opposite end, more abstract representations, such as motion flows represented by arrows \cite{yin2023dragnuwa, 2023videocomposer} and vectors \cite{2023videocomposer} or motion themes like ‘inflate,’ ‘squish,’ or ‘crumble’ \cite{Pikaffect}, are used as motion guidance. These methods offer greater generalization across various object classes while compromising on the control over specific motion details and relative frame positioning.

This paper proposes a modular video generation pipeline with a bounding-box driven motion transfer implementation. The method seeks to explore a midpoint in the spectrum of motion control, offering an alternative perspective and demonstrating its generative potential. The goal is to retain control over relative frame positions and overall motion from the driving video, rather than requiring detailed, instance-aligned spatial conditioning sequences. This approach aims to enable more flexible and realistic adaptations of motion characteristics in video generation.
