@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@inproceedings{10.1145/383259.383295,
author = {Hertzmann, Aaron and Jacobs, Charles E. and Oliver, Nuria and Curless, Brian and Salesin, David H.},
title = {Image analogies},
year = {2001},
isbn = {158113374X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383259.383295},
doi = {10.1145/383259.383295},
abstract = {This paper describes a new framework for processing images by example, called “image analogies.” The framework involves two stages: a design phase, in which a pair of images, with one image purported to be a “filtered” version of the other, is presented as “training data”; and an application phase, in which the learned filter is applied to some new target image in order to create an “analogous” filtered result. Image analogies are based on a simple multi-scale autoregression, inspired primarily by recent results in texture synthesis. By choosing different types of source image pairs as input, the framework supports a wide variety of “image filter” effects, including traditional image filters, such as blurring or embossing; improved texture synthesis, in which some textures are synthesized with higher quality than by previous approaches; super-resolution, in which a higher-resolution image is inferred from a low-resolution source; texture transfer, in which images are “texturized” with some arbitrary source texture; artistic filters, in which various drawing and painting styles are synthesized based on scanned real-world examples; and texture-by-numbers, in which realistic scenes, composed of a variety of textures, are created using a simple painting interface.},
booktitle = {Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {327–340},
numpages = {14},
keywords = {Markov random fields, autoregression, example-based rendering, non-photorealistic rendering, texture synthesis, texture transfer, texture-by-numbers},
series = {SIGGRAPH '01}
}

@misc{ho2020denoisingdiffusionprobabilisticmodels,
      title={Denoising Diffusion Probabilistic Models}, 
      author={Jonathan Ho and Ajay Jain and Pieter Abbeel},
      year={2020},
      eprint={2006.11239},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.11239}, 
}

@misc{ho2022videodiffusionmodels,
      title={Video Diffusion Models}, 
      author={Jonathan Ho and Tim Salimans and Alexey Gritsenko and William Chan and Mohammad Norouzi and David J. Fleet},
      year={2022},
      eprint={2204.03458},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.03458}, 
}

@article{Halperin_2021,
   title={Endless loops: detecting and animating periodic patterns in still images},
   volume={40},
   ISSN={1557-7368},
   url={http://dx.doi.org/10.1145/3450626.3459935},
   DOI={10.1145/3450626.3459935},
   number={4},
   journal={ACM Transactions on Graphics},
   publisher={Association for Computing Machinery (ACM)},
   author={Halperin, Tavi and Hakim, Hanit and Vantzos, Orestis and Hochman, Gershon and Benaim, Netai and Sassy, Lior and Kupchik, Michael and Bibi, Ofir and Fried, Ohad},
   year={2021},
   month=jul, pages={1–12} }


@misc{dorkenwald2021stochasticimagetovideosynthesisusing,
      title={Stochastic Image-to-Video Synthesis using cINNs}, 
      author={Michael Dorkenwald and Timo Milbich and Andreas Blattmann and Robin Rombach and Konstantinos G. Derpanis and Björn Ommer},
      year={2021},
      eprint={2105.04551},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2105.04551}, 
}

@misc{hu2024animateanyoneconsistentcontrollable,
      title={Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation}, 
      author={Li Hu and Xin Gao and Peng Zhang and Ke Sun and Bang Zhang and Liefeng Bo},
      year={2024},
      eprint={2311.17117},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.17117}, 
}

@inproceedings{52750,
title	= {DreamPose: Fashion Video Synthesis with Stable Diffusion},author	= {Johanna Karras and Aleksander Hołyński and Ting-Chun Wang and Ira Kemelmacher-Shlizerman},year	= {2023},URL	= {https://grail.cs.washington.edu/projects/dreampose/}}

@misc{blattmann2023stablevideodiffusionscaling,
      title={Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets}, 
      author={Andreas Blattmann and Tim Dockhorn and Sumith Kulal and Daniel Mendelevitch and Maciej Kilian and Dominik Lorenz and Yam Levi and Zion English and Vikram Voleti and Adam Letts and Varun Jampani and Robin Rombach},
      year={2023},
      eprint={2311.15127},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.15127}, 
}

@misc{ho2022videodiffusionmodels,
      title={Video Diffusion Models}, 
      author={Jonathan Ho and Tim Salimans and Alexey Gritsenko and William Chan and Mohammad Norouzi and David J. Fleet},
      year={2022},
      eprint={2204.03458},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.03458}, 
}

@misc{melnik2024videodiffusionmodelssurvey,
      title={Video Diffusion Models: A Survey}, 
      author={Andrew Melnik and Michal Ljubljanac and Cong Lu and Qi Yan and Weiming Ren and Helge Ritter},
      year={2024},
      eprint={2405.03150},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.03150}, 
}

@misc{zhang2023addingconditionalcontroltexttoimage,
      title={Adding Conditional Control to Text-to-Image Diffusion Models}, 
      author={Lvmin Zhang and Anyi Rao and Maneesh Agrawala},
      year={2023},
      eprint={2302.05543},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2302.05543}, 
}

@misc{zhang2023controlvideotrainingfreecontrollabletexttovideo,
      title={ControlVideo: Training-free Controllable Text-to-Video Generation}, 
      author={Yabo Zhang and Yuxiang Wei and Dongsheng Jiang and Xiaopeng Zhang and Wangmeng Zuo and Qi Tian},
      year={2023},
      eprint={2305.13077},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.13077}, 
}

@misc{wang2023videocomposercompositionalvideosynthesis,
      title={VideoComposer: Compositional Video Synthesis with Motion Controllability}, 
      author={Xiang Wang and Hangjie Yuan and Shiwei Zhang and Dayou Chen and Jiuniu Wang and Yingya Zhang and Yujun Shen and Deli Zhao and Jingren Zhou},
      year={2023},
      eprint={2306.02018},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2306.02018}, 
}

@misc{wang2024easycontroltransfercontrolnetvideo,
      title={EasyControl: Transfer ControlNet to Video Diffusion for Controllable Generation and Interpolation}, 
      author={Cong Wang and Jiaxi Gu and Panwen Hu and Haoyu Zhao and Yuanfan Guo and Jianhua Han and Hang Xu and Xiaodan Liang},
      year={2024},
      eprint={2408.13005},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.13005}, 
}

@misc{shi2024motioni2vconsistentcontrollableimagetovideo,
      title={Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling}, 
      author={Xiaoyu Shi and Zhaoyang Huang and Fu-Yun Wang and Weikang Bian and Dasong Li and Yi Zhang and Manyuan Zhang and Ka Chun Cheung and Simon See and Hongwei Qin and Jifeng Dai and Hongsheng Li},
      year={2024},
      eprint={2401.15977},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.15977}, 
}