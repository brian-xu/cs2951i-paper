@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
  author = {FirstName LastName},
  title  = {The frobnicatable foo filter},
  note   = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
  year   = 2014
}

@misc{Authors14b,
  author = {FirstName LastName},
  title  = {Frobnication tutorial},
  note   = {Supplied as supplemental material {\tt tr.pdf}},
  year   = 2014
}

@article{Alpher02,
  author  = {FirstName Alpher},
  title   = {Frobnication},
  journal = PAMI,
  volume  = 12,
  number  = 1,
  pages   = {234--778},
  year    = 2002
}

@article{Alpher03,
  author  = {FirstName Alpher and  FirstName Fotheringham-Smythe},
  title   = {Frobnication revisited},
  journal = {Journal of Foo},
  volume  = 13,
  number  = 1,
  pages   = {234--778},
  year    = 2003
}

@article{Alpher04,
  author  = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
  title   = {Can a machine frobnicate?},
  journal = {Journal of Foo},
  volume  = 14,
  number  = 1,
  pages   = {234--778},
  year    = 2004
}

@inproceedings{Alpher05,
  author    = {FirstName Alpher and FirstName Gamow},
  title     = {Can a computer frobnicate?},
  booktitle = CVPR,
  pages     = {234--778},
  year      = 2005
}

@article{ravi2024sam2,
  title   = {SAM 2: Segment Anything in Images and Videos},
  author  = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph},
  journal = {arXiv preprint arXiv:2408.00714},
  url     = {https://arxiv.org/abs/2408.00714},
  year    = {2024}
}

@article{2023videocomposer,
  title   = {VideoComposer: Compositional Video Synthesis with Motion Controllability},
  author  = {{Wang, Xiang and Yuan, Hangjie and Zhang, Shiwei and Chen, Dayou and Wang, Jiuniu, and Zhang, Yingya, and Shen, Yujun, and Zhao, Deli and Zhou, Jingren}},
  journal = {arXiv preprint arXiv:2306.02018},
  url     = {https://arxiv.org/abs/2306.02018},
  year    = {2023}
}

@article{2001hertzman,
  author    = {Hertzmann, Aaron and Jacobs, Charles E. and Oliver, Nuria and Curless, Brian and Salesin, David H.},
  title     = {Image analogies},
  year      = {2001},
  isbn      = {158113374X},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/383259.383295},
  doi       = {10.1145/383259.383295},
  abstract  = {This paper describes a new framework for processing images by example, called “image analogies.” The framework involves two stages: a design phase, in which a pair of images, with one image purported to be a “filtered” version of the other, is presented as “training data”; and an application phase, in which the learned filter is applied to some new target image in order to create an “analogous” filtered result. Image analogies are based on a simple multi-scale autoregression, inspired primarily by recent results in texture synthesis. By choosing different types of source image pairs as input, the framework supports a wide variety of “image filter” effects, including traditional image filters, such as blurring or embossing; improved texture synthesis, in which some textures are synthesized with higher quality than by previous approaches; super-resolution, in which a higher-resolution image is inferred from a low-resolution source; texture transfer, in which images are “texturized” with some arbitrary source texture; artistic filters, in which various drawing and painting styles are synthesized based on scanned real-world examples; and texture-by-numbers, in which realistic scenes, composed of a variety of textures, are created using a simple painting interface.},
  booktitle = {Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques},
  pages     = {327–340},
  numpages  = {14},
  keywords  = {Markov random fields, autoregression, example-based rendering, non-photorealistic rendering, texture synthesis, texture transfer, texture-by-numbers},
  series    = {SIGGRAPH '01}
}

@misc{ho2020denoisingdiffusionprobabilisticmodels,
  title         = {Denoising Diffusion Probabilistic Models},
  author        = {Jonathan Ho and Ajay Jain and Pieter Abbeel},
  year          = {2020},
  eprint        = {2006.11239},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2006.11239}
}

@article{Halperin_2021,
  title     = {Endless loops: detecting and animating periodic patterns in still images},
  volume    = {40},
  issn      = {1557-7368},
  url       = {http://dx.doi.org/10.1145/3450626.3459935},
  doi       = {10.1145/3450626.3459935},
  number    = {4},
  journal   = {ACM Transactions on Graphics},
  publisher = {Association for Computing Machinery (ACM)},
  author    = {Halperin, Tavi and Hakim, Hanit and Vantzos, Orestis and Hochman, Gershon and Benaim, Netai and Sassy, Lior and Kupchik, Michael and Bibi, Ofir and Fried, Ohad},
  year      = {2021},
  month     = jul,
  pages     = {1–12}
}


@misc{dorkenwald2021stochasticimagetovideosynthesisusing,
  title         = {Stochastic Image-to-Video Synthesis using cINNs},
  author        = {Michael Dorkenwald and Timo Milbich and Andreas Blattmann and Robin Rombach and Konstantinos G. Derpanis and Björn Ommer},
  year          = {2021},
  eprint        = {2105.04551},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2105.04551}
}

@misc{hu2024animateanyoneconsistentcontrollable,
  title         = {Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation},
  author        = {Li Hu and Xin Gao and Peng Zhang and Ke Sun and Bang Zhang and Liefeng Bo},
  year          = {2024},
  eprint        = {2311.17117},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2311.17117}
}

@inproceedings{52750,
  title  = {DreamPose: Fashion Video Synthesis with Stable Diffusion},
  author = {Johanna Karras and Aleksander Hołyński and Ting-Chun Wang and Ira Kemelmacher-Shlizerman},
  year   = {2023},
  url    = {https://grail.cs.washington.edu/projects/dreampose/}
}

@misc{blattmann2023stablevideodiffusionscaling,
  title         = {Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets},
  author        = {Andreas Blattmann and Tim Dockhorn and Sumith Kulal and Daniel Mendelevitch and Maciej Kilian and Dominik Lorenz and Yam Levi and Zion English and Vikram Voleti and Adam Letts and Varun Jampani and Robin Rombach},
  year          = {2023},
  eprint        = {2311.15127},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2311.15127}
}

@misc{ho2022videodiffusionmodels,
  title         = {Video Diffusion Models},
  author        = {Jonathan Ho and Tim Salimans and Alexey Gritsenko and William Chan and Mohammad Norouzi and David J. Fleet},
  year          = {2022},
  eprint        = {2204.03458},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2204.03458}
}

@misc{melnik2024videodiffusionmodelssurvey,
  title         = {Video Diffusion Models: A Survey},
  author        = {Andrew Melnik and Michal Ljubljanac and Cong Lu and Qi Yan and Weiming Ren and Helge Ritter},
  year          = {2024},
  eprint        = {2405.03150},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2405.03150}
}

@misc{zhang2023addingconditionalcontroltexttoimage,
  title         = {Adding Conditional Control to Text-to-Image Diffusion Models},
  author        = {Lvmin Zhang and Anyi Rao and Maneesh Agrawala},
  year          = {2023},
  eprint        = {2302.05543},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2302.05543}
}

@misc{zhang2023controlvideotrainingfreecontrollabletexttovideo,
  title         = {ControlVideo: Training-free Controllable Text-to-Video Generation},
  author        = {Yabo Zhang and Yuxiang Wei and Dongsheng Jiang and Xiaopeng Zhang and Wangmeng Zuo and Qi Tian},
  year          = {2023},
  eprint        = {2305.13077},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2305.13077}
}

@misc{wang2023videocomposercompositionalvideosynthesis,
  title         = {VideoComposer: Compositional Video Synthesis with Motion Controllability},
  author        = {Xiang Wang and Hangjie Yuan and Shiwei Zhang and Dayou Chen and Jiuniu Wang and Yingya Zhang and Yujun Shen and Deli Zhao and Jingren Zhou},
  year          = {2023},
  eprint        = {2306.02018},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2306.02018}
}

@misc{wang2024easycontroltransfercontrolnetvideo,
  title         = {EasyControl: Transfer ControlNet to Video Diffusion for Controllable Generation and Interpolation},
  author        = {Cong Wang and Jiaxi Gu and Panwen Hu and Haoyu Zhao and Yuanfan Guo and Jianhua Han and Hang Xu and Xiaodan Liang},
  year          = {2024},
  eprint        = {2408.13005},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2408.13005}
}

@misc{shi2024motioni2vconsistentcontrollableimagetovideo,
  title         = {Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling},
  author        = {Xiaoyu Shi and Zhaoyang Huang and Fu-Yun Wang and Weikang Bian and Dasong Li and Yi Zhang and Manyuan Zhang and Ka Chun Cheung and Simon See and Hongwei Qin and Jifeng Dai and Hongsheng Li},
  year          = {2024},
  eprint        = {2401.15977},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2401.15977}
}

@article{feng2023dreamoving,
    title     = {DreaMoving: A Human Video Generation Framework based on Diffusion Models},
    author    = {Mengyang Feng and Jinlin Liu and Kai Yu and Yuan Yao and Zheng Hui and Xiefan Guo and Xianhui Lin and Haolan Xue and Chen Shi and Xiaowen Li and Aojie Li and Xiaoyang Kang and Biwen Lei and Miaomiao Cui and Peiran Ren and Xuansong Xie},
    journal   = {arXiv},
    year      = {2023}
}

@inproceedings{ni2023conditional,
  title       = {Conditional image-to-video generation with latent flow diffusion models},
  author      = {Ni, Haomiao and Shi, Changhao and Li, Kai and Huang, Sharon X and Min, Martin Renqiang},
  booktitle   = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages       = {18444--18455},
  year        = {2023}
}

@article{yin2023dragnuwa,
  title       = {Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory},
  author      = {Yin, Shengming and Wu, Chenfei and Liang, Jian and Shi, Jie and Li, Houqiang and Ming, Gong and Duan, Nan},
  journal     = {arXiv preprint arXiv:2308.08089},
  year        = {2023}
}

@online{Pikaffect,
  author      = {Eric Hal Schwartz},
  title       = {This AI video generator can melt, crush, blow up, or turn anything into cake},
  year        = {2024},
  url         = {https://www.techradar.com/computing/artificial-intelligence/pika-15-takes-ai-video-making-beyond-physics},
  urldate     = {2024-12-18}
}

@article{luo2024diffusion,
  title       = {Diffusion hyperfeatures: Searching through time and space for semantic correspondence},
  author      = {Luo, Grace and Dunlap, Lisa and Park, Dong Huk and Holynski, Aleksander and Darrell, Trevor},
  journal     = {Advances in Neural Information Processing Systems},
  volume      = {36},
  year        = {2024}
}

@InProceedings{Dutt_2024_CVPR,
    author    = {Dutt, Niladri Shekhar and Muralikrishnan, Sanjeev and Mitra, Niloy J.},
    title     = {Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {4494-4504}
}   
